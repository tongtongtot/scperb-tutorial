SpaPerb, equipped with the variational auto-encoders to estimate the latent content variable, and the dataset-related style, generate new samples from the control and stimulus dataset:
$$
\begin{gather}
Z_{ctrl}=E(X_{ctrl})\\
Z_{stim}=E(X_{stim})\\
\end{gather}
$$

Then we applied the decoder to generate the new data sample based on the extracted feature:
SpaPerb aims to transfer the gene expressions from the control dataset to the stimulus dataset. 

There are two ways to achieve this aim. 

The first way is to make the latent representation of the control and stimulus data as close as possible. Inspired by the style transfer GAN, we style-transferred the latent representation of the control data into stimulus data. We add a delta vector $Z_{delta}$ to the $Z_{ctrl}$ to achieve the goal. The delta vector $Z_{delta}$ is generated by projecting a delta vector $V_{delta}$ from a delta encoder $E_{delta}$. By using a delta encoder, $Z_{delta}$ is able to learn the difference between $Z_{ctrl}$ and $Z_{stimulated}$ much better than using a constant vector. Finally, to decrease the bias of the delta vector, $V_{delta}$ is generated by randomly choosing values that ranged $(0,1)$ instead of using a vector with constant values. 

Therefore, the process is shown below:
$$
\begin{gather}
V_{delta} = random(input_-size)\\  
Z_{delta}=E^{delta}(V_{delta})\\  
X_{stim}'=D(Z_{ctrl} + Z_{delta})\\  
L_{delta} = criterion(Z_{delta}, (Z_{stim}-Z_{ctrl}))\\
\end{gather}
$$

Another way is to make the generated image similar to the stimulus dataset. 

This process differs from the original VAE and other methods. As these methods apply a reconstruction loss, that is, to compute a loss between $X_{ctrl},X_{ctrl}'$ and $X_{stim},X_{stim}'$. 

In this case, there are two losses:
$$
\begin{gather}
X_{ctrl}'=D(Z_{ctrl}) \\  
X_{stim}'=D(Z_{stim}) \\  
L_{ctrl}=criterion(X_{ctrl}, X_{ctrl}')\\  
L_{stim}=criterion(X_{stim}, X_{stim}')\\
\end{gather}
$$
However, in this case, we want to emphasize the style transfer from $X_{ctrl}$ to $X_{stim}$, the reconstructed data $X_{stimulated}'$ should be similar to the original data $X_{stimulated}$, so the reconstruction loss will be:
$$
\begin{gather}
X_{stim}'=D(Z_{ctrl}) \\  
L_{reconstruction}=criterion(X_{stim}, X_{stim}')\\
\end{gather}
$$
In our model, SpaPerb, we combine these two methods above and generated a better result than only using one of these models. In our model, we first randomly chose a vector $V_{delta}$ with a size of input from $(0,1)$; then project it through a delta encoder $E_{delta}$ to get the latent representation of the difference between control and stimulus datasets; finally, we compute a reconstruction loss between the generated and the real stimulus images. 

Then by using VAE, we need to know the possibility curve of the graph; that is, we need to maximize $\displaystyle\sum_{x} P(x)$, or in other words, $\displaystyle\sum_x log(P(x))$.

Therefore, we have to transform this equation to solve this problem. Here, we create a new function $f$ to help us better achieve the goal:
$$
\begin{align*}
log(P(x)) =& \ log(P(x)) \times \displaystyle\int f(z|x)dz \\
=& \ \displaystyle\int f(z|x)\times log(\frac{P(z,x)}{P(z|x)}) dz \\
=& \ \displaystyle\int f(z|x)\times log(\frac{P(z,x)\times f(z|x)}{f(z|x) \times P(z|x)})dz \\
=& \ \displaystyle\int f(z|x)\times log(\frac{P(z,x)}{f(z|x)}) dz \ + \displaystyle\int f(z|x)\times log(\frac{f(z|x)}{P(z|x)}) dz\\
=& \ \displaystyle\int f(z|x)\times log(\frac{P(z,x)}{f(z|x)}) dz \ + KLD(f(z|x)||P(z|x))\\
\end{align*}
$$
In this case, as $KLD(f(z|x)||P(z|x)) \ge 0$,  $log(P(x)) \ge \displaystyle\int f(z|x)\times log(\frac{P(z,x)}{f(z|x)}) dz$.

Also, as $P(x) = \displaystyle\int P(x|z)P(z)dz$, $P(x)$ is independent to $f(x)$, so when optimizing $f(z|x)$ to minimize $KLD(f(z|x)||P(z|x))$, the $log(P(x))$ will gradually become closer to $\displaystyle\int f(z|x)\times log(\frac{P(z,x)}{f(z|x)}) dz$, or in other words, the lower bound of $log(P(x))$.

Therefore, in order to maximize $log(P(x))$, we need to maximize the lower bound $\displaystyle\int f(z|x)\times log(\frac{P(z,x)}{f(z|x)}) dz$.

To maximize the lower bound, we need further processing:
$$
\begin{align*}
Lowerbound =& \displaystyle\int f(z|x)\times log(\frac{P(z,x)}{f(z|x)}) dz\\
=& \displaystyle\int f(z|x)\times log(\frac{P(x|z)\times P(z)}{f(z|x)}) dz\\
=& -\displaystyle\int f(z|x)\times log(\frac{f(z|x)}{P(x|z)}) dz \ + \displaystyle\int f(z|x)\times log(\frac{P(z)}{f(z|x)}) dz\\
=& -KLD(f(z|x)||P(z))\ + E_{f(z|x)}[log(P(x|z))]
\end{align*}
$$
Therefore, $log(P(x)) = KLD(f(z|x)||P(z|x))- KLD(f(z|x)||P(z)) + E_{f(z|x)}[log(P(x|z))]$.

By transforming the equation above, we can get the final equation to compute the loss for the model.
$$
\begin{align*}
log(P(x)) - KLD(f(z|x)||P(z|x)) =& - KLD(f(z|x)||P(z)) + E_{f(z|x)}[log(P(x|z))]\\
=& \ ELBO\\
\end{align*}
$$
And our goal is to construct a neural network to compute a function f to maximize ELBO.

By setting a prior distribution $P(z)$ as a normal distribution $N(0,1)$, we can use a neuro network to generate $\mu$ and $\sigma$ of $f(z|x)$ and maximizing the KLD part of the ELBO by using a Kullback-Leibler divergence $L_{KL} =  - KLD(N(\mu,\sigma),N(0,I))$; Maximizing the reconstruction loss $L_{reconstruct}$ will maximize the rest part of the ELBO.

As a result, the total loss of SpaPerb will be:
$$
Loss = w1 \times L_{latent} + w2 \times L_{reconstruct} + w3\times L_{KL}
$$
In SpaPerb the weight sets up as the following: $w1=100,w2=100,w3=0.5$.

Overall, the process works as the following:
$$
\begin{gather}
V_{delta} = random(input_-size)\\  
Z_{delta}=E^{delta}(V_{delta})\\  
Z_{ctrl}=E(X_{ctrl}) \\  
Z_{stim}=E(X_{stim}) \\  
Z_{stim}'=Z_{ctrl} + Z_{delta}\\  
X_{stim}'=D(Z_{stim}')\\  
L_{delta} = Smooth_-L1Loss(Z_{delta}, (Z_{stim}-Z_{ctrl}))\\  
L_{reconstruction} = Smooth_-L1Loss(X_{stim}, X_{stim}')\\  
L_{KL} = KLD(P_x, N(0,1))\\  
Loss=100 \times L_{reconstruct} + 100 \times L_{latent} + 0.5\times L_{KL}\\
\end{gather}
$$
