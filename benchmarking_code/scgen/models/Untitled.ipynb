{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f333e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from scipy import sparse\n",
    "\n",
    "from util import balancer, extractor, shuffle_data\n",
    "\n",
    "# log = logging.getLogger(__file__)\n",
    "\n",
    "\n",
    "class VAEArith:\n",
    "    \"\"\"\n",
    "        VAE with Arithmetic vector Network class. This class contains the implementation of Variational\n",
    "        Auto-encoder network with Vector Arithmetics.\n",
    "\n",
    "        # Parameters\n",
    "            kwargs:\n",
    "                key: `validation_data` : AnnData\n",
    "                    must be fed if `use_validation` is true.\n",
    "                key: `dropout_rate`: float\n",
    "                        dropout rate\n",
    "                key: `learning_rate`: float\n",
    "                    learning rate of optimization algorithm\n",
    "                key: `model_path`: basestring\n",
    "                    path to save the model after training\n",
    "            x_dimension: integer\n",
    "                number of gene expression space dimensions.\n",
    "            z_dimension: integer\n",
    "                number of latent space dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_dimension, z_dimension=100, **kwargs):\n",
    "        tf.reset_default_graph()\n",
    "        self.x_dim = x_dimension\n",
    "        self.z_dim = z_dimension\n",
    "        self.learning_rate = kwargs.get(\"learning_rate\", 0.001)\n",
    "        self.dropout_rate = kwargs.get(\"dropout_rate\", 0.2)\n",
    "        self.model_to_use = kwargs.get(\"model_path\", \"./models/scgen\")\n",
    "        self.alpha = kwargs.get(\"alpha\", 0.001)\n",
    "        self.is_training = tf.placeholder(tf.bool, name='training_flag')\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False, dtype=tf.int32)\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, self.x_dim], name=\"data\")\n",
    "        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\"latent\")\n",
    "        self.time_step = tf.placeholder(tf.int32)\n",
    "        self.size = tf.placeholder(tf.int32)\n",
    "        self.init_w = tf.contrib.layers.xavier_initializer()\n",
    "        self._create_network()\n",
    "        self._loss_function()\n",
    "        self.sess = tf.Session()\n",
    "        self.saver = tf.train.Saver(max_to_keep=1)\n",
    "        self.init = tf.global_variables_initializer().run(session=self.sess)\n",
    "\n",
    "    def _encoder(self):\n",
    "        \"\"\"\n",
    "            Constructs the encoder sub-network of VAE. This function implements the\n",
    "            encoder part of Variational Auto-encoder. It will transform primary\n",
    "            data in the `n_vars` dimension-space to the `z_dimension` latent space.\n",
    "\n",
    "            # Parameters\n",
    "                No parameters are needed.\n",
    "\n",
    "            # Returns\n",
    "                mean: Tensor\n",
    "                    A dense layer consists of means of gaussian distributions of latent space dimensions.\n",
    "                log_var: Tensor\n",
    "                    A dense layer consists of log transformed variances of gaussian distributions of latent space dimensions.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):\n",
    "            h = tf.layers.dense(inputs=self.x, units=800, kernel_initializer=self.init_w, use_bias=False)\n",
    "            h = tf.layers.batch_normalization(h, axis=1, training=self.is_training)\n",
    "            h = tf.nn.leaky_relu(h)\n",
    "            h = tf.layers.dropout(h, self.dropout_rate, training=self.is_training)\n",
    "            h = tf.layers.dense(inputs=h, units=800, kernel_initializer=self.init_w, use_bias=False)\n",
    "            h = tf.layers.batch_normalization(h, axis=1, training=self.is_training)\n",
    "            h = tf.nn.leaky_relu(h)\n",
    "            h = tf.layers.dropout(h, self.dropout_rate, training=self.is_training)\n",
    "            mean = tf.layers.dense(inputs=h, units=self.z_dim, kernel_initializer=self.init_w)\n",
    "            log_var = tf.layers.dense(inputs=h, units=self.z_dim, kernel_initializer=self.init_w)\n",
    "            return mean, log_var\n",
    "\n",
    "    def _decoder(self):\n",
    "        \"\"\"\n",
    "            Constructs the decoder sub-network of VAE. This function implements the\n",
    "            decoder part of Variational Auto-encoder. It will transform constructed\n",
    "            latent space to the previous space of data with n_dimensions = n_vars.\n",
    "\n",
    "            # Parameters\n",
    "                No parameters are needed.\n",
    "\n",
    "            # Returns\n",
    "                h: Tensor\n",
    "                    A Tensor for last dense layer with the shape of [n_vars, ] to reconstruct data.\n",
    "\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"decoder\", reuse=tf.AUTO_REUSE):\n",
    "            h = tf.layers.dense(inputs=self.z_mean, units=800, kernel_initializer=self.init_w, use_bias=False)\n",
    "            h = tf.layers.batch_normalization(h, axis=1, training=self.is_training)\n",
    "            h = tf.nn.leaky_relu(h)\n",
    "            h = tf.layers.dropout(h, self.dropout_rate, training=self.is_training)\n",
    "            h = tf.layers.dense(inputs=h, units=800, kernel_initializer=self.init_w, use_bias=False)\n",
    "            tf.layers.batch_normalization(h, axis=1, training=self.is_training)\n",
    "            h = tf.nn.leaky_relu(h)\n",
    "            h = tf.layers.dropout(h, self.dropout_rate, training=self.is_training)\n",
    "            h = tf.layers.dense(inputs=h, units=self.x_dim, kernel_initializer=self.init_w, use_bias=True)\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "\n",
    "    def _sample_z(self):\n",
    "        \"\"\"\n",
    "            Samples from standard Normal distribution with shape [size, z_dim] and\n",
    "            applies re-parametrization trick. It is actually sampling from latent\n",
    "            space distributions with N(mu, var) computed in `_encoder` function.\n",
    "\n",
    "            # Parameters\n",
    "                No parameters are needed.\n",
    "\n",
    "            # Returns\n",
    "                The computed Tensor of samples with shape [size, z_dim].\n",
    "        \"\"\"\n",
    "        eps = tf.random_normal(shape=[self.size, self.z_dim])\n",
    "        return self.mu + tf.exp(self.log_var / 2) * eps\n",
    "\n",
    "    def _create_network(self):\n",
    "        \"\"\"\n",
    "            Constructs the whole VAE network. It is step-by-step constructing the VAE\n",
    "            network. First, It will construct the encoder part and get mu, log_var of\n",
    "            latent space. Second, It will sample from the latent space to feed the\n",
    "            decoder part in next step. Finally, It will reconstruct the data by\n",
    "            constructing decoder part of VAE.\n",
    "\n",
    "            # Parameters\n",
    "                No parameters are needed.\n",
    "\n",
    "            # Returns\n",
    "                Nothing will be returned.\n",
    "        \"\"\"\n",
    "        self.mu, self.log_var = self._encoder()\n",
    "        self.z_mean = self._sample_z()\n",
    "        self.x_hat = self._decoder()\n",
    "\n",
    "    def _loss_function(self):\n",
    "        \"\"\"\n",
    "            Defines the loss function of VAE network after constructing the whole\n",
    "            network. This will define the KL Divergence and Reconstruction loss for\n",
    "            VAE and also defines the Optimization algorithm for network. The VAE Loss\n",
    "            will be weighted sum of reconstruction loss and KL Divergence loss.\n",
    "\n",
    "            # Parameters\n",
    "                No parameters are needed.\n",
    "\n",
    "            # Returns\n",
    "                Nothing will be returned.\n",
    "        \"\"\"\n",
    "        kl_loss = 0.5 * tf.reduce_sum(\n",
    "            tf.exp(self.log_var) + tf.square(self.mu) - 1. - self.log_var, 1)\n",
    "        recon_loss = 0.5 * tf.reduce_sum(tf.square((self.x - self.x_hat)), 1)\n",
    "        self.vae_loss = tf.reduce_mean(recon_loss + self.alpha * kl_loss)\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            self.solver = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.vae_loss)\n",
    "\n",
    "    def to_latent(self, data):\n",
    "        \"\"\"\n",
    "            Map `data` in to the latent space. This function will feed data\n",
    "            in encoder part of VAE and compute the latent space coordinates\n",
    "            for each sample in data.\n",
    "\n",
    "            # Parameters\n",
    "                data:  numpy nd-array\n",
    "                    Numpy nd-array to be mapped to latent space. `data.X` has to be in shape [n_obs, n_vars].\n",
    "\n",
    "            # Returns\n",
    "                latent: numpy nd-array\n",
    "                    Returns array containing latent space encoding of 'data'\n",
    "        \"\"\"\n",
    "        latent = self.sess.run(self.z_mean, feed_dict={self.x: data, self.size: data.shape[0], self.is_training: False})\n",
    "        return latent\n",
    "\n",
    "    def _avg_vector(self, data):\n",
    "        \"\"\"\n",
    "            Computes the average of points which computed from mapping `data`\n",
    "            to encoder part of VAE.\n",
    "\n",
    "            # Parameters\n",
    "                data:  numpy nd-array\n",
    "                    Numpy nd-array matrix to be mapped to latent space. Note that `data.X` has to be in shape [n_obs, n_vars].\n",
    "\n",
    "            # Returns\n",
    "                The average of latent space mapping in numpy nd-array.\n",
    "\n",
    "        \"\"\"\n",
    "        latent = self.to_latent(data)\n",
    "        latent_avg = numpy.average(latent, axis=0)\n",
    "        return latent_avg\n",
    "\n",
    "    def reconstruct(self, data, use_data=False):\n",
    "        \"\"\"\n",
    "            Map back the latent space encoding via the decoder.\n",
    "\n",
    "            # Parameters\n",
    "                data: `~anndata.AnnData`\n",
    "                    Annotated data matrix whether in latent space or gene expression space.\n",
    "                use_data: bool\n",
    "                    This flag determines whether the `data` is already in latent space or not.\n",
    "                    if `True`: The `data` is in latent space (`data.X` is in shape [n_obs, z_dim]).\n",
    "                    if `False`: The `data` is not in latent space (`data.X` is in shape [n_obs, n_vars]).\n",
    "\n",
    "            # Returns\n",
    "                rec_data: 'numpy nd-array'\n",
    "                    Returns 'numpy nd-array` containing reconstructed 'data' in shape [n_obs, n_vars].\n",
    "        \"\"\"\n",
    "        if use_data:\n",
    "            latent = data\n",
    "        else:\n",
    "            latent = self.to_latent(data)\n",
    "        rec_data = self.sess.run(self.x_hat, feed_dict={self.z_mean: latent, self.is_training: False})\n",
    "        return rec_data\n",
    "\n",
    "    def linear_interpolation(self, source_adata, dest_adata, n_steps):\n",
    "        \"\"\"\n",
    "            Maps `source_adata` and `dest_adata` into latent space and linearly interpolate\n",
    "            `n_steps` points between them.\n",
    "\n",
    "            # Parameters\n",
    "                source_adata: `~anndata.AnnData`\n",
    "                    Annotated data matrix of source cells in gene expression space (`x.X` must be in shape [n_obs, n_vars])\n",
    "                dest_adata: `~anndata.AnnData`\n",
    "                    Annotated data matrix of destinations cells in gene expression space (`y.X` must be in shape [n_obs, n_vars])\n",
    "                n_steps: int\n",
    "                    Number of steps to interpolate points between `source_adata`, `dest_adata`.\n",
    "\n",
    "            # Returns\n",
    "                interpolation: numpy nd-array\n",
    "                    Returns the `numpy nd-array` of interpolated points in gene expression space.\n",
    "\n",
    "            # Example\n",
    "            ```python\n",
    "                import anndata\n",
    "                import scgen\n",
    "                train_data = anndata.read(\"./data/train.h5ad\")\n",
    "                validation_data = anndata.read(\"./data/validation.h5ad\")\n",
    "                network = scgen.VAEArith(x_dimension= train_data.shape[1], model_path=\"./models/test\" )\n",
    "                network.train(train_data=train_data, use_validation=True, valid_data=validation_data, shuffle=True, n_epochs=2)\n",
    "                souece = train_data[((train_data.obs[\"cell_type\"] == \"CD8T\") & (train_data.obs[\"condition\"] == \"control\"))]\n",
    "                destination = train_data[((train_data.obs[\"cell_type\"] == \"CD8T\") & (train_data.obs[\"condition\"] == \"stimulated\"))]\n",
    "                interpolation = network.linear_interpolation(souece, destination, n_steps=25)\n",
    "            ```\n",
    "        \"\"\"\n",
    "        if sparse.issparse(source_adata.X):\n",
    "            source_average = source_adata.X.A.mean(axis=0).reshape((1, source_adata.shape[1]))\n",
    "        else:\n",
    "            source_average = source_adata.X.mean(axis=0).reshape((1, source_adata.shape[1]))\n",
    "\n",
    "        if sparse.issparse(dest_adata.X):\n",
    "            dest_average = dest_adata.X.A.mean(axis=0).reshape((1, dest_adata.shape[1]))\n",
    "        else:\n",
    "            dest_average = dest_adata.X.mean(axis=0).reshape((1, dest_adata.shape[1]))\n",
    "        start = self.to_latent(source_average)\n",
    "        end = self.to_latent(dest_average)\n",
    "        vectors = numpy.zeros((n_steps, start.shape[1]))\n",
    "        alpha_values = numpy.linspace(0, 1, n_steps)\n",
    "        for i, alpha in enumerate(alpha_values):\n",
    "            vector = start * (1 - alpha) + end * alpha\n",
    "            vectors[i, :] = vector\n",
    "        vectors = numpy.array(vectors)\n",
    "        interpolation = self.reconstruct(vectors, use_data=True)\n",
    "        return interpolation\n",
    "\n",
    "    def predict(self, adata, conditions, cell_type_key, condition_key, adata_to_predict=None, celltype_to_predict=None,\n",
    "                obs_key=\"all\", biased=False):\n",
    "        \"\"\"\n",
    "            Predicts the cell type provided by the user in stimulated condition.\n",
    "\n",
    "            # Parameters\n",
    "                celltype_to_predict: basestring\n",
    "                    The cell type you want to be predicted.\n",
    "                obs_key: basestring or dict\n",
    "                    Dictionary of celltypes you want to be observed for prediction.\n",
    "                adata_to_predict: `~anndata.AnnData`\n",
    "                    Adata for unpertubed cells you want to be predicted.\n",
    "\n",
    "            # Returns\n",
    "                predicted_cells: numpy nd-array\n",
    "                    `numpy nd-array` of predicted cells in primary space.\n",
    "                delta: float\n",
    "                    Difference between stimulated and control cells in latent space\n",
    "\n",
    "            # Example\n",
    "            ```python\n",
    "                import anndata\n",
    "                import scgen\n",
    "                train_data = anndata.read(\"./data/train.h5ad\"\n",
    "                validation_data = anndata.read(\"./data/validation.h5ad\")\n",
    "                network = scgen.VAEArith(x_dimension= train_data.shape[1], model_path=\"./models/test\" )\n",
    "                network.train(train_data=train_data, use_validation=True, valid_data=validation_data, shuffle=True, n_epochs=2)\n",
    "                prediction, delta = network.predict(adata= train_data, celltype_to_predict= \"CD4T\", conditions={\"ctrl\": \"control\", \"stim\": \"stimulated\"})\n",
    "            ```\n",
    "        \"\"\"\n",
    "        if obs_key == \"all\":\n",
    "            ctrl_x = adata[adata.obs[condition_key] == conditions[\"ctrl\"], :]\n",
    "            stim_x = adata[adata.obs[condition_key] == conditions[\"stim\"], :]\n",
    "            if not biased:\n",
    "                ctrl_x = balancer(ctrl_x, cell_type_key=cell_type_key, condition_key=condition_key)\n",
    "                stim_x = balancer(stim_x, cell_type_key=cell_type_key, condition_key=condition_key)\n",
    "        else:\n",
    "            key = list(obs_key.keys())[0]\n",
    "            values = obs_key[key]\n",
    "            subset = adata[adata.obs[key].isin(values)]\n",
    "            ctrl_x = subset[subset.obs[condition_key] == conditions[\"ctrl\"], :]\n",
    "            stim_x = subset[subset.obs[condition_key] == conditions[\"stim\"], :]\n",
    "            if len(values) > 1 and not biased:\n",
    "                ctrl_x = balancer(ctrl_x, cell_type_key=cell_type_key, condition_key=condition_key)\n",
    "                stim_x = balancer(stim_x, cell_type_key=cell_type_key, condition_key=condition_key)\n",
    "        if celltype_to_predict is not None and adata_to_predict is not None:\n",
    "            raise Exception(\"Please provide either a cell type or adata not both!\")\n",
    "        if celltype_to_predict is None and adata_to_predict is None:\n",
    "            raise Exception(\"Please provide a cell type name or adata for your unperturbed cells\")\n",
    "        if celltype_to_predict is not None:\n",
    "            ctrl_pred = extractor(adata, celltype_to_predict, conditions, cell_type_key, condition_key)[1]\n",
    "        else:\n",
    "            ctrl_pred = adata_to_predict\n",
    "        if not biased:\n",
    "            eq = min(ctrl_x.X.shape[0], stim_x.X.shape[0])\n",
    "            cd_ind = numpy.random.choice(range(ctrl_x.shape[0]), size=eq, replace=False)\n",
    "            stim_ind = numpy.random.choice(range(stim_x.shape[0]), size=eq, replace=False)\n",
    "        else:\n",
    "            cd_ind = numpy.random.choice(range(ctrl_x.shape[0]), size=ctrl_x.shape[0], replace=False)\n",
    "            stim_ind = numpy.random.choice(range(stim_x.shape[0]), size=stim_x.shape[0], replace=False)\n",
    "        if sparse.issparse(ctrl_x.X) and sparse.issparse(stim_x.X):\n",
    "            latent_ctrl = self._avg_vector(ctrl_x.X.A[cd_ind, :])\n",
    "            latent_sim = self._avg_vector(stim_x.X.A[stim_ind, :])\n",
    "        else:\n",
    "            latent_ctrl = self._avg_vector(ctrl_x.X[cd_ind, :])\n",
    "            latent_sim = self._avg_vector(stim_x.X[stim_ind, :])\n",
    "        delta = latent_sim - latent_ctrl\n",
    "        if sparse.issparse(ctrl_pred.X):\n",
    "            latent_cd = self.to_latent(ctrl_pred.X.A)\n",
    "        else:\n",
    "            latent_cd = self.to_latent(ctrl_pred.X)\n",
    "        stim_pred = delta + latent_cd\n",
    "        predicted_cells = self.reconstruct(stim_pred, use_data=True)\n",
    "        return predicted_cells, delta\n",
    "\n",
    "    def predict_cross(self, train, data, conditions):\n",
    "        cd_x = train.copy()[train.obs[\"condition\"] == conditions[\"ctrl\"], :]\n",
    "        cd_x = balancer(cd_x)\n",
    "        stim_x = train.copy()[train.obs[\"condition\"] == conditions[\"stim\"], :]\n",
    "        stim_x = balancer(stim_x)\n",
    "        cd_y = data.copy()\n",
    "        eq = min(cd_x.X.shape[0], stim_x.X.shape[0])\n",
    "        cd_ind = numpy.random.choice(range(cd_x.shape[0]), size=eq, replace=False)\n",
    "        stim_ind = numpy.random.choice(range(stim_x.shape[0]), size=eq, replace=False)\n",
    "        lat_cd = self._avg_vector(cd_x.X[cd_ind, :])\n",
    "        lat_stim = self._avg_vector(stim_x.X[stim_ind, :])\n",
    "        delta = lat_stim - lat_cd\n",
    "        latent_cd = self.to_latent(cd_y.X)\n",
    "        stim_pred = delta + latent_cd\n",
    "        predicted_cells = self.reconstruct(stim_pred, use_data=True)\n",
    "        return predicted_cells, delta\n",
    "\n",
    "    def restore_model(self):\n",
    "        \"\"\"\n",
    "            restores model weights from `model_to_use`.\n",
    "\n",
    "            # Parameters\n",
    "                No parameters are needed.\n",
    "\n",
    "            # Returns\n",
    "                Nothing will be returned.\n",
    "\n",
    "            # Example\n",
    "            ```python\n",
    "                import anndata\n",
    "                import scgen\n",
    "                train_data = anndata.read(\"./data/train.h5ad\")\n",
    "                validation_data = anndata.read(\"./data/validation.h5ad\")\n",
    "                network = scgen.VAEArith(x_dimension= train_data.shape[1], model_path=\"./models/test\" )\n",
    "                network.restore_model()\n",
    "            ```\n",
    "        \"\"\"\n",
    "        self.saver.restore(self.sess, self.model_to_use)\n",
    "\n",
    "    def train(self, train_data, use_validation=False, valid_data=None, n_epochs=25, batch_size=32, early_stop_limit=20,\n",
    "              threshold=0.00025, initial_run=True, shuffle=True, save=True):\n",
    "        \"\"\"\n",
    "            Trains the network `n_epochs` times with given `train_data`\n",
    "            and validates the model using validation_data if it was given\n",
    "            in the constructor function. This function is using `early stopping`\n",
    "            technique to prevent over-fitting.\n",
    "\n",
    "            # Parameters\n",
    "                train_data: scanpy AnnData\n",
    "                    Annotated Data Matrix for training VAE network.\n",
    "                use_validation: bool\n",
    "                    if `True`: must feed a valid AnnData object to `valid_data` argument.\n",
    "                valid_data: scanpy AnnData\n",
    "                    Annotated Data Matrix for validating VAE network after each epoch.\n",
    "                n_epochs: int\n",
    "                    Number of epochs to iterate and optimize network weights\n",
    "                batch_size: integer\n",
    "                    size of each batch of training dataset to be fed to network while training.\n",
    "                early_stop_limit: int\n",
    "                    Number of consecutive epochs in which network loss is not going lower.\n",
    "                    After this limit, the network will stop training.\n",
    "                threshold: float\n",
    "                    Threshold for difference between consecutive validation loss values\n",
    "                    if the difference is upper than this `threshold`, this epoch will not\n",
    "                    considered as an epoch in early stopping.\n",
    "                initial_run: bool\n",
    "                    if `True`: The network will initiate training and log some useful initial messages.\n",
    "                    if `False`: Network will resume the training using `restore_model` function in order\n",
    "                        to restore last model which has been trained with some training dataset.\n",
    "                shuffle: bool\n",
    "                    if `True`: shuffles the training dataset\n",
    "\n",
    "            # Returns\n",
    "                Nothing will be returned\n",
    "\n",
    "            # Example\n",
    "            ```python\n",
    "                import anndata\n",
    "                import scgen\n",
    "                train_data = anndata.read(\"./data/train.h5ad\"\n",
    "                validation_data = anndata.read(\"./data/validation.h5ad\"\n",
    "                network = scgen.VAEArith(x_dimension= train_data.shape[1], model_path=\"./models/test\")\n",
    "                network.train(train_data=train_data, use_validation=True, valid_data=validation_data, shuffle=True, n_epochs=2)\n",
    "            ```\n",
    "        \"\"\"\n",
    "        if initial_run:\n",
    "            log.info(\"----Training----\")\n",
    "            assign_step_zero = tf.assign(self.global_step, 0)\n",
    "            _init_step = self.sess.run(assign_step_zero)\n",
    "        if not initial_run:\n",
    "            self.saver.restore(self.sess, self.model_to_use)\n",
    "        if use_validation and valid_data is None:\n",
    "            raise Exception(\"valid_data is None but use_validation is True.\")\n",
    "        if shuffle:\n",
    "            train_data = shuffle_data(train_data)\n",
    "        loss_hist = []\n",
    "        patience = early_stop_limit\n",
    "        min_delta = threshold\n",
    "        patience_cnt = 0\n",
    "        for it in range(n_epochs):\n",
    "            increment_global_step_op = tf.assign(self.global_step, self.global_step + 1)\n",
    "            _step = self.sess.run(increment_global_step_op)\n",
    "            current_step = self.sess.run(self.global_step)\n",
    "            train_loss = 0.0\n",
    "            for lower in range(0, train_data.shape[0], batch_size):\n",
    "                upper = min(lower + batch_size, train_data.shape[0])\n",
    "                if sparse.issparse(train_data.X):\n",
    "                    x_mb = train_data[lower:upper, :].X.A\n",
    "                else:\n",
    "                    x_mb = train_data[lower:upper, :].X\n",
    "                if upper - lower > 1:\n",
    "                    _, current_loss_train = self.sess.run([self.solver, self.vae_loss],\n",
    "                                                          feed_dict={self.x: x_mb, self.time_step: current_step,\n",
    "                                                                     self.size: len(x_mb), self.is_training: True})\n",
    "                    train_loss += current_loss_train\n",
    "            if use_validation:\n",
    "                valid_loss = 0\n",
    "                for lower in range(0, valid_data.shape[0], batch_size):\n",
    "                    upper = min(lower + batch_size, valid_data.shape[0])\n",
    "                    if sparse.issparse(valid_data.X):\n",
    "                        x_mb = valid_data[lower:upper, :].X.A\n",
    "                    else:\n",
    "                        x_mb = valid_data[lower:upper, :].X\n",
    "                    current_loss_valid = self.sess.run(self.vae_loss,\n",
    "                                                       feed_dict={self.x: x_mb, self.time_step: current_step,\n",
    "                                                                  self.size: len(x_mb), self.is_training: False})\n",
    "                    valid_loss += current_loss_valid\n",
    "                loss_hist.append(valid_loss / valid_data.shape[0])\n",
    "                if it > 0 and loss_hist[it - 1] - loss_hist[it] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                if patience_cnt > patience:\n",
    "                    save_path = self.saver.save(self.sess, self.model_to_use)\n",
    "                    break\n",
    "            print(f\"Epoch {it}: Train Loss: {train_loss / (train_data.shape[0] // batch_size)},\\t Validation Loss: {valid_loss / (valid_data.shape[0] // batch_size)}\")\n",
    "        if save:\n",
    "            os.makedirs(self.model_to_use, exist_ok=True)\n",
    "            save_path = self.saver.save(self.sess, self.model_to_use)\n",
    "            log.info(f\"Model saved in file: {save_path}. Training finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f08d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scgen\n",
      "  Using cached scgen-2.1.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: adjustText in /opt/conda/lib/python3.7/site-packages (from scgen) (0.8)\n",
      "Requirement already satisfied: anndata>=0.7.5 in /opt/conda/lib/python3.7/site-packages (from scgen) (0.8.0)\n",
      "Collecting importlib-metadata<2.0,>=1.0 (from scgen)\n",
      "  Using cached importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: scanpy>=1.6 in /opt/conda/lib/python3.7/site-packages (from scgen) (1.9.3)\n",
      "Collecting scvi-tools>=0.15.0 (from scgen)\n",
      "  Using cached scvi_tools-0.19.0-py3-none-any.whl (296 kB)\n",
      "Requirement already satisfied: seaborn>=0.11 in /opt/conda/lib/python3.7/site-packages (from scgen) (0.12.2)\n",
      "Requirement already satisfied: pandas>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from anndata>=0.7.5->scgen) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from anndata>=0.7.5->scgen) (1.18.5)\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from anndata>=0.7.5->scgen) (1.7.3)\n",
      "Collecting h5py>=3 (from anndata>=0.7.5->scgen)\n",
      "  Using cached h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Requirement already satisfied: natsort in /opt/conda/lib/python3.7/site-packages (from anndata>=0.7.5->scgen) (8.3.1)\n",
      "Requirement already satisfied: packaging>=20 in /opt/conda/lib/python3.7/site-packages (from anndata>=0.7.5->scgen) (23.1)\n",
      "Requirement already satisfied: typing_extensions in /home/tony/.local/lib/python3.7/site-packages (from anndata>=0.7.5->scgen) (4.6.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<2.0,>=1.0->scgen) (3.15.0)\n",
      "Requirement already satisfied: matplotlib>=3.4 in /opt/conda/lib/python3.7/site-packages (from scanpy>=1.6->scgen) (3.5.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from scanpy>=1.6->scgen) (4.64.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.7/site-packages (from scanpy>=1.6->scgen) (1.0.2)\n",
      "Requirement already satisfied: statsmodels>=0.10.0rc2 in /opt/conda/lib/python3.7/site-packages (from scanpy>=1.6->scgen) (0.13.5)\n",
      "Requirement already satisfied: patsy in /opt/conda/lib/python3.7/site-packages (from scanpy>=1.6->scgen) (0.5.3)\n",
      "Requirement already satisfied: networkx>=2.3 in /opt/conda/lib/python3.7/site-packages (from scanpy>=1.6->scgen) (2.6.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from scanpy>=1.6->scgen) (1.2.0)\n",
      "Requirement already satisfied: numba>=0.41.0 in /opt/conda/lib/python3.7/site-packages (from scanpy>=1.6->scgen) (0.56.4)\n",
      "Requirement already satisfied: umap-learn>=0.3.10 in /opt/conda/lib/python3.7/site-packages (from scanpy>=1.6->scgen) (0.5.3)\n",
      "Requirement already satisfied: session-info in /opt/conda/lib/python3.7/site-packages (from scanpy>=1.6->scgen) (1.0.0)\n",
      "Collecting chex (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached chex-0.1.5-py3-none-any.whl (85 kB)\n",
      "Collecting docrep>=0.3.2 (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached docrep-0.3.2-py3-none-any.whl\n",
      "Collecting flax (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached flax-0.6.10-py3-none-any.whl (226 kB)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.7/site-packages (from scvi-tools>=0.15.0->scgen) (8.0.6)\n",
      "Collecting jax>=0.3 (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached jax-0.3.25-py3-none-any.whl\n",
      "Collecting jaxlib (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached jaxlib-0.3.25-cp37-cp37m-manylinux2014_x86_64.whl (71.2 MB)\n",
      "Collecting ml-collections>=0.1.1 (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached ml_collections-0.1.1-py3-none-any.whl\n",
      "Collecting mudata>=0.1.2 (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached mudata-0.2.1-py3-none-any.whl (23 kB)\n",
      "Collecting numpyro (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached numpyro-0.12.1-py3-none-any.whl (304 kB)\n",
      "Collecting openpyxl>=3.0 (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "Collecting optax (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached optax-0.1.4-py3-none-any.whl (154 kB)\n",
      "Collecting pyro-ppl>=1.6.0 (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached pyro_ppl-1.8.5-py3-none-any.whl (732 kB)\n",
      "Collecting pytorch-lightning<1.8,>=1.7.0 (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached pytorch_lightning-1.7.7-py3-none-any.whl (708 kB)\n",
      "Requirement already satisfied: rich>=9.1.0 in /opt/conda/lib/python3.7/site-packages (from scvi-tools>=0.15.0->scgen) (13.3.5)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/tony/.local/lib/python3.7/site-packages (from scvi-tools>=0.15.0->scgen) (1.13.1)\n",
      "Collecting torchmetrics>=0.6.0 (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from docrep>=0.3.2->scvi-tools>=0.15.0->scgen) (1.16.0)\n",
      "Collecting numpy>=1.16.5 (from anndata>=0.7.5->scgen)\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum in /opt/conda/lib/python3.7/site-packages (from jax>=0.3->scvi-tools>=0.15.0->scgen) (3.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.4->scanpy>=1.6->scgen) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.4->scanpy>=1.6->scgen) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.4->scanpy>=1.6->scgen) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.4->scanpy>=1.6->scgen) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.4->scanpy>=1.6->scgen) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.4->scanpy>=1.6->scgen) (2.8.2)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from ml-collections>=0.1.1->scvi-tools>=0.15.0->scgen) (1.4.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from ml-collections>=0.1.1->scvi-tools>=0.15.0->scgen) (6.0)\n",
      "Collecting contextlib2 (from ml-collections>=0.1.1->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba>=0.41.0->scanpy>=1.6->scgen) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba>=0.41.0->scanpy>=1.6->scgen) (67.7.2)\n",
      "Collecting et-xmlfile (from openpyxl>=3.0->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1.1->anndata>=0.7.5->scgen) (2022.7)\n",
      "Collecting pyro-api>=0.1.1 (from pyro-ppl>=1.6.0->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
      "INFO: pip is looking at multiple versions of pyro-ppl to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyro-ppl>=1.6.0 (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached pyro_ppl-1.8.4-py3-none-any.whl (730 kB)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (2023.1.0)\n",
      "Collecting tensorboard>=2.9.1 (from pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Collecting pyDeprecate>=0.3.1 (from pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from rich>=9.1.0->scvi-tools>=0.15.0->scgen) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from rich>=9.1.0->scvi-tools>=0.15.0->scgen) (2.15.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22->scanpy>=1.6->scgen) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/tony/.local/lib/python3.7/site-packages (from torch>=1.8.0->scvi-tools>=0.15.0->scgen) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/tony/.local/lib/python3.7/site-packages (from torch>=1.8.0->scvi-tools>=0.15.0->scgen) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/tony/.local/lib/python3.7/site-packages (from torch>=1.8.0->scvi-tools>=0.15.0->scgen) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/tony/.local/lib/python3.7/site-packages (from torch>=1.8.0->scvi-tools>=0.15.0->scgen) (11.7.99)\n",
      "Requirement already satisfied: wheel in /home/tony/.local/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.0->scvi-tools>=0.15.0->scgen) (0.40.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.7/site-packages (from umap-learn>=0.3.10->scanpy>=1.6->scgen) (0.5.10)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from chex->scvi-tools>=0.15.0->scgen) (0.1.8)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from chex->scvi-tools>=0.15.0->scgen) (0.12.0)\n",
      "INFO: pip is looking at multiple versions of flax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting flax (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached flax-0.6.9-py3-none-any.whl (226 kB)\n",
      "  Using cached flax-0.6.8-py3-none-any.whl (216 kB)\n",
      "  Using cached flax-0.6.7-py3-none-any.whl (214 kB)\n",
      "  Using cached flax-0.6.6-py3-none-any.whl (210 kB)\n",
      "  Using cached flax-0.6.4-py3-none-any.whl (204 kB)\n",
      "Requirement already satisfied: msgpack in /opt/conda/lib/python3.7/site-packages (from flax->scvi-tools>=0.15.0->scgen) (1.0.5)\n",
      "Collecting orbax (from flax->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached orbax-0.1.0-py3-none-any.whl (66 kB)\n",
      "Collecting tensorstore (from flax->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached tensorstore-0.1.28-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets->scvi-tools>=0.15.0->scgen) (6.16.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets->scvi-tools>=0.15.0->scgen) (7.33.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets->scvi-tools>=0.15.0->scgen) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /opt/conda/lib/python3.7/site-packages (from ipywidgets->scvi-tools>=0.15.0->scgen) (4.0.7)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /opt/conda/lib/python3.7/site-packages (from ipywidgets->scvi-tools>=0.15.0->scgen) (3.0.7)\n",
      "INFO: pip is looking at multiple versions of numpyro to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpyro (from scvi-tools>=0.15.0->scgen)\n",
      "  Using cached numpyro-0.12.0-py3-none-any.whl (304 kB)\n",
      "  Using cached numpyro-0.11.0-py3-none-any.whl (300 kB)\n",
      "  Using cached numpyro-0.10.1-py3-none-any.whl (292 kB)\n",
      "Collecting multipledispatch (from numpyro->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: stdlib-list in /opt/conda/lib/python3.7/site-packages (from session-info->scanpy>=1.6->scgen) (0.8.0)\n",
      "Requirement already satisfied: requests in /home/tony/.local/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (3.8.4)\n",
      "Requirement already satisfied: debugpy>=1.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->scvi-tools>=0.15.0->scgen) (1.6.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->scvi-tools>=0.15.0->scgen) (7.4.9)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->scvi-tools>=0.15.0->scgen) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->scvi-tools>=0.15.0->scgen) (1.5.6)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->scvi-tools>=0.15.0->scgen) (5.9.3)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->scvi-tools>=0.15.0->scgen) (24.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->scvi-tools>=0.15.0->scgen) (6.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets->scvi-tools>=0.15.0->scgen) (0.18.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets->scvi-tools>=0.15.0->scgen) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets->scvi-tools>=0.15.0->scgen) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets->scvi-tools>=0.15.0->scgen) (3.0.38)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets->scvi-tools>=0.15.0->scgen) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets->scvi-tools>=0.15.0->scgen) (4.8.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.7/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=9.1.0->scvi-tools>=0.15.0->scgen) (0.1.2)\n",
      "INFO: pip is looking at multiple versions of pynndescent to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pynndescent>=0.5 (from umap-learn>=0.3.10->scanpy>=1.6->scgen)\n",
      "  Using cached pynndescent-0.5.9.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached pynndescent-0.5.8.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached pynndescent-0.5.7-py3-none-any.whl\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (1.51.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (2.17.3)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (3.4.3)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (3.20.3)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (2.2.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cached_property (from orbax->flax->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.7/site-packages (from orbax->flax->scvi-tools>=0.15.0->scgen) (5.12.0)\n",
      "Collecting etils (from orbax->flax->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached etils-0.9.0-py3-none-any.whl (140 kB)\n",
      "Collecting pytest (from orbax->flax->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached pytest-7.3.1-py3-none-any.whl (320 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/tony/.local/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (1.3.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (0.13.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->scvi-tools>=0.15.0->scgen) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->scvi-tools>=0.15.0->scgen) (0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->scvi-tools>=0.15.0->scgen) (4.11.1)\n",
      "INFO: pip is looking at multiple versions of markdown to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting markdown>=2.6.8 (from tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached Markdown-3.4.2-py3-none-any.whl (93 kB)\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "  Using cached Markdown-3.4-py3-none-any.whl (93 kB)\n",
      "  Using cached Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->scvi-tools>=0.15.0->scgen) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets->scvi-tools>=0.15.0->scgen) (0.2.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tony/.local/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tony/.local/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (2.1.1)\n",
      "Collecting iniconfig (from pytest->orbax->flax->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest->orbax->flax->scvi-tools>=0.15.0->scgen) (1.0.0)\n",
      "Collecting exceptiongroup>=1.0.0rc8 (from pytest->orbax->flax->scvi-tools>=0.15.0->scgen)\n",
      "  Using cached exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest->orbax->flax->scvi-tools>=0.15.0->scgen) (2.0.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning<1.8,>=1.7.0->scvi-tools>=0.15.0->scgen) (3.2.2)\n",
      "Installing collected packages: tensorboard-plugin-wit, pyro-api, cached_property, tensorboard-data-server, pyDeprecate, numpy, multipledispatch, iniconfig, importlib-metadata, exceptiongroup, etils, et-xmlfile, docrep, contextlib2, tensorstore, openpyxl, ml-collections, markdown, h5py, pytest, jaxlib, jax, google-auth-oauthlib, torchmetrics, tensorboard, pyro-ppl, pynndescent, numpyro, mudata, chex, pytorch-lightning, optax, orbax, flax, scvi-tools, scgen\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.11.4\n",
      "    Uninstalling importlib-metadata-4.11.4:\n",
      "      Successfully uninstalled importlib-metadata-4.11.4\n",
      "  Attempting uninstall: markdown\n",
      "    Found existing installation: Markdown 3.4.3\n",
      "    Uninstalling Markdown-3.4.3:\n",
      "      Successfully uninstalled Markdown-3.4.3\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.0.0\n",
      "    Uninstalling google-auth-oauthlib-1.0.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 1.15.0\n",
      "    Uninstalling tensorboard-1.15.0:\n",
      "      Successfully uninstalled tensorboard-1.15.0\n",
      "  Attempting uninstall: pynndescent\n",
      "    Found existing installation: pynndescent 0.5.10\n",
      "    Uninstalling pynndescent-0.5.10:\n",
      "      Successfully uninstalled pynndescent-0.5.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "gymnasium 0.26.3 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 1.7.0 which is incompatible.\r\n",
      "jupyterlab-server 2.22.1 requires importlib-metadata>=4.8.3; python_version < \"3.10\", but you have importlib-metadata 1.7.0 which is incompatible.\r\n",
      "keyring 23.13.1 requires importlib-metadata>=4.11.4; python_version < \"3.12\", but you have importlib-metadata 1.7.0 which is incompatible.\r\n",
      "nbconvert 7.3.1 requires importlib-metadata>=3.6; python_version < \"3.10\", but you have importlib-metadata 1.7.0 which is incompatible.\r\n",
      "nbformat 5.8.0 requires importlib-metadata>=3.6; python_version < \"3.8\", but you have importlib-metadata 1.7.0 which is incompatible.\r\n",
      "opentelemetry-api 1.17.0 requires importlib-metadata~=6.0.0, but you have importlib-metadata 1.7.0 which is incompatible.\r\n",
      "tensorflow 1.15.5 requires h5py<=2.10.0, but you have h5py 3.8.0 which is incompatible.\r\n",
      "tensorflow 1.15.5 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.21.6 which is incompatible.\r\n",
      "tensorflow 1.15.5 requires tensorboard<1.16.0,>=1.15.0, but you have tensorboard 2.11.2 which is incompatible.\r\n",
      "virtualenv 20.21.0 requires importlib-metadata>=4.8.3; python_version < \"3.8\", but you have importlib-metadata 1.7.0 which is incompatible.\r\n",
      "ydata-profiling 4.1.2 requires requests<2.29,>=2.24.0, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed cached_property-1.5.2 chex-0.1.5 contextlib2-21.6.0 docrep-0.3.2 et-xmlfile-1.1.0 etils-0.9.0 exceptiongroup-1.1.1 flax-0.6.4 google-auth-oauthlib-0.4.6 h5py-3.8.0 importlib-metadata-1.7.0 iniconfig-2.0.0 jax-0.3.25 jaxlib-0.3.25 markdown-3.3.4 ml-collections-0.1.1 mudata-0.2.1 multipledispatch-0.6.0 numpy-1.21.6 numpyro-0.10.1 openpyxl-3.1.2 optax-0.1.4 orbax-0.1.0 pyDeprecate-0.3.2 pynndescent-0.5.7 pyro-api-0.1.2 pyro-ppl-1.8.4 pytest-7.3.1 pytorch-lightning-1.7.7 scgen-2.1.0 scvi-tools-0.19.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorstore-0.1.28 torchmetrics-0.11.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip install scgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "640371e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_latent(data):\n",
    "        \"\"\"\n",
    "            Map `data` in to the latent space. This function will feed data\n",
    "            in encoder part of VAE and compute the latent space coordinates\n",
    "            for each sample in data.\n",
    "\n",
    "            # Parameters\n",
    "                data:  numpy nd-array\n",
    "                    Numpy nd-array to be mapped to latent space. `data.X` has to be in shape [n_obs, n_vars].\n",
    "\n",
    "            # Returns\n",
    "                latent: numpy nd-array\n",
    "                    Returns array containing latent space encoding of 'data'\n",
    "        \"\"\"\n",
    "        sess = tf.Session()\n",
    "        latent = sess.run(z_mean, feed_dict={x: data, size: data.shape[0], is_training: False})\n",
    "        return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f2a8689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _avg_vector(data):\n",
    "        \"\"\"\n",
    "            Computes the average of points which computed from mapping `data`\n",
    "            to encoder part of VAE.\n",
    "\n",
    "            # Parameters\n",
    "                data:  numpy nd-array\n",
    "                    Numpy nd-array matrix to be mapped to latent space. Note that `data.X` has to be in shape [n_obs, n_vars].\n",
    "\n",
    "            # Returns\n",
    "                The average of latent space mapping in numpy nd-array.\n",
    "\n",
    "        \"\"\"\n",
    "        latent = to_latent(data)\n",
    "        latent_avg = numpy.average(latent, axis=0)\n",
    "        return latent_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3450ceb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/anndata/compat/__init__.py:235: FutureWarning: Moving element from .uns['neighbors']['distances'] to .obsp['distances'].\n",
      "\n",
      "This is where adjacency matrices should go now.\n",
      "  FutureWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/anndata/compat/__init__.py:235: FutureWarning: Moving element from .uns['neighbors']['connectivities'] to .obsp['connectivities'].\n",
      "\n",
      "This is where adjacency matrices should go now.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "train = sc.read(\"train.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d2136b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_11764/593066687.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_avg_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/tmp/ipykernel_11764/1997766643.py\u001b[0m in \u001b[0;36m_avg_vector\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \"\"\"\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlatent_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlatent_avg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/tmp/ipykernel_11764/2561310125.py\u001b[0m in \u001b[0;36mto_latent\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \"\"\"\n\u001b[1;32m     15\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'z_mean' is not defined"
     ]
    }
   ],
   "source": [
    "_avg_vector(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64020cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
